{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nXE7vqRPBMhL"
      },
      "source": [
        "# Improvement on Prompt-to-Prompt Image Editing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F2mDYx_X-5nP"
      },
      "outputs": [],
      "source": [
        "!pip install diffusers==0.6.0\n",
        "!pip install transformers==4.24.0 -i https://pypi.python.org/simple\n",
        "# !pip install accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BO7hi-oiBMhO"
      },
      "outputs": [],
      "source": [
        "from typing import Optional, Union, Tuple, List, Callable, Dict\n",
        "import torch\n",
        "from diffusers import StableDiffusionPipeline\n",
        "import torch.nn.functional as nnf\n",
        "import numpy as np\n",
        "import abc\n",
        "import ptp_utils\n",
        "import seq_aligner\n",
        "from ptp_utils import *\n",
        "from seq_aligner import *\n",
        "from tqdm.notebook import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hjqc94U7BMhQ"
      },
      "outputs": [],
      "source": [
        "MY_TOKEN = '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'\n",
        "LOW_RESOURCE = True\n",
        "NUM_DIFFUSION_STEPS = 50\n",
        "GUIDANCE_SCALE = 7.5\n",
        "MAX_NUM_WORDS = 77\n",
        "device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
        "ldm_stable = StableDiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\", use_auth_token=MY_TOKEN).to(device)\n",
        "tokenizer = ldm_stable.tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1QlIhU8Z-bM8"
      },
      "source": [
        "## Google's Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uPZ0gGUJBMhT"
      },
      "outputs": [],
      "source": [
        "class LocalBlend:\n",
        "\n",
        "    def __call__(self, x_t, attention_store):\n",
        "        k = 1\n",
        "        maps = attention_store[\"down_cross\"][2:4] + attention_store[\"up_cross\"][:3]\n",
        "        maps = [item.reshape(self.alpha_layers.shape[0], -1, 1, 16, 16, MAX_NUM_WORDS) for item in maps]\n",
        "        maps = torch.cat(maps, dim=1)\n",
        "        maps = (maps * self.alpha_layers).sum(-1).mean(1)\n",
        "        mask = nnf.max_pool2d(maps, (k * 2 + 1, k * 2 +1), (1, 1), padding=(k, k))\n",
        "        mask = nnf.interpolate(mask, size=(x_t.shape[2:]))\n",
        "        mask = mask / mask.max(2, keepdims=True)[0].max(3, keepdims=True)[0]\n",
        "        mask = mask.gt(self.threshold)\n",
        "        mask = (mask[:1] + mask[1:]).float()\n",
        "        x_t = x_t[:1] + mask * (x_t - x_t[:1])\n",
        "        return x_t\n",
        "       \n",
        "    def __init__(self, prompts: List[str], words: [List[List[str]]], threshold=.3):\n",
        "        alpha_layers = torch.zeros(len(prompts),  1, 1, 1, 1, MAX_NUM_WORDS)\n",
        "        for i, (prompt, words_) in enumerate(zip(prompts, words)):\n",
        "            if type(words_) is str:\n",
        "                words_ = [words_]\n",
        "            for word in words_:\n",
        "                ind = ptp_utils.get_word_inds(prompt, word, tokenizer)\n",
        "                alpha_layers[i, :, :, :, :, ind] = 1\n",
        "        self.alpha_layers = alpha_layers.to(device)\n",
        "        self.threshold = threshold\n",
        "\n",
        "\n",
        "class AttentionControl(abc.ABC):\n",
        "    \n",
        "    def step_callback(self, x_t):\n",
        "        return x_t\n",
        "    \n",
        "    def between_steps(self):\n",
        "        return\n",
        "    \n",
        "    @property\n",
        "    def num_uncond_att_layers(self):\n",
        "        return self.num_att_layers if LOW_RESOURCE else 0\n",
        "    \n",
        "    @abc.abstractmethod\n",
        "    def forward (self, attn, is_cross: bool, place_in_unet: str):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def __call__(self, attn, is_cross: bool, place_in_unet: str):\n",
        "        if self.cur_att_layer >= self.num_uncond_att_layers:\n",
        "            if LOW_RESOURCE:\n",
        "                attn = self.forward(attn, is_cross, place_in_unet)\n",
        "            else:\n",
        "                h = attn.shape[0]\n",
        "                attn[h // 2:] = self.forward(attn[h // 2:], is_cross, place_in_unet)\n",
        "        self.cur_att_layer += 1\n",
        "        if self.cur_att_layer == self.num_att_layers + self.num_uncond_att_layers:\n",
        "            self.cur_att_layer = 0\n",
        "            self.cur_step += 1\n",
        "            self.between_steps()\n",
        "        return attn\n",
        "    \n",
        "    def reset(self):\n",
        "        self.cur_step = 0\n",
        "        self.cur_att_layer = 0\n",
        "\n",
        "    def __init__(self):\n",
        "        self.cur_step = 0\n",
        "        self.num_att_layers = -1\n",
        "        self.cur_att_layer = 0\n",
        "\n",
        "class EmptyControl(AttentionControl):\n",
        "    \n",
        "    def forward (self, attn, is_cross: bool, place_in_unet: str):\n",
        "        return attn\n",
        "    \n",
        "    \n",
        "class AttentionStore(AttentionControl):\n",
        "\n",
        "    @staticmethod\n",
        "    def get_empty_store():\n",
        "        return {\"down_cross\": [], \"mid_cross\": [], \"up_cross\": [],\n",
        "                \"down_self\": [],  \"mid_self\": [],  \"up_self\": []}\n",
        "\n",
        "    def forward(self, attn, is_cross: bool, place_in_unet: str):\n",
        "        key = f\"{place_in_unet}_{'cross' if is_cross else 'self'}\"\n",
        "        if attn.shape[1] <= 32 ** 2:  # avoid memory overhead\n",
        "            self.step_store[key].append(attn)\n",
        "        return attn\n",
        "\n",
        "    def between_steps(self):\n",
        "        if len(self.attention_store) == 0:\n",
        "            self.attention_store = self.step_store\n",
        "        else:\n",
        "            for key in self.attention_store:\n",
        "                for i in range(len(self.attention_store[key])):\n",
        "                    self.attention_store[key][i] += self.step_store[key][i]\n",
        "        self.step_store = self.get_empty_store()\n",
        "\n",
        "    def get_average_attention(self):\n",
        "        average_attention = {key: [item / self.cur_step for item in self.attention_store[key]] for key in self.attention_store}\n",
        "        return average_attention\n",
        "\n",
        "\n",
        "    def reset(self):\n",
        "        super(AttentionStore, self).reset()\n",
        "        self.step_store = self.get_empty_store()\n",
        "        self.attention_store = {}\n",
        "\n",
        "    def __init__(self):\n",
        "        super(AttentionStore, self).__init__()\n",
        "        self.step_store = self.get_empty_store()\n",
        "        self.attention_store = {}\n",
        "\n",
        "        \n",
        "class AttentionControlEdit(AttentionStore, abc.ABC):\n",
        "    \n",
        "    def step_callback(self, x_t):\n",
        "        if self.local_blend is not None:\n",
        "            x_t = self.local_blend(x_t, self.attention_store)\n",
        "        return x_t\n",
        "        \n",
        "    def replace_self_attention(self, attn_base, att_replace):\n",
        "        if att_replace.shape[2] <= 16 ** 2:\n",
        "            return attn_base.unsqueeze(0).expand(att_replace.shape[0], *attn_base.shape)\n",
        "        else:\n",
        "            return att_replace\n",
        "    \n",
        "    @abc.abstractmethod\n",
        "    def replace_cross_attention(self, attn_base, att_replace):\n",
        "        raise NotImplementedError\n",
        "    \n",
        "    def forward(self, attn, is_cross: bool, place_in_unet: str):\n",
        "        super(AttentionControlEdit, self).forward(attn, is_cross, place_in_unet)\n",
        "        if is_cross or (self.num_self_replace[0] <= self.cur_step < self.num_self_replace[1]):\n",
        "            h = attn.shape[0] // (self.batch_size)\n",
        "            attn = attn.reshape(self.batch_size, h, *attn.shape[1:])\n",
        "            attn_base, attn_repalce = attn[0], attn[1:]\n",
        "            if is_cross:\n",
        "                alpha_words = self.cross_replace_alpha[self.cur_step]\n",
        "                attn_repalce_new = self.replace_cross_attention(attn_base, attn_repalce) * alpha_words + (1 - alpha_words) * attn_repalce\n",
        "                attn[1:] = attn_repalce_new\n",
        "            else:\n",
        "                attn[1:] = self.replace_self_attention(attn_base, attn_repalce)\n",
        "            attn = attn.reshape(self.batch_size * h, *attn.shape[2:])\n",
        "        return attn\n",
        "    \n",
        "    def __init__(self, prompts, num_steps: int,\n",
        "                 cross_replace_steps: Union[float, Tuple[float, float], Dict[str, Tuple[float, float]]],\n",
        "                 self_replace_steps: Union[float, Tuple[float, float]],\n",
        "                 local_blend: Optional[LocalBlend]):\n",
        "        super(AttentionControlEdit, self).__init__()\n",
        "        self.batch_size = len(prompts)\n",
        "        self.cross_replace_alpha = ptp_utils.get_time_words_attention_alpha(prompts, num_steps, cross_replace_steps, tokenizer).to(device)\n",
        "        if type(self_replace_steps) is float:\n",
        "            self_replace_steps = 0, self_replace_steps\n",
        "        self.num_self_replace = int(num_steps * self_replace_steps[0]), int(num_steps * self_replace_steps[1])\n",
        "        self.local_blend = local_blend\n",
        "\n",
        "class AttentionReplace(AttentionControlEdit):\n",
        "\n",
        "    def replace_cross_attention(self, attn_base, att_replace):\n",
        "        return torch.einsum('hpw,bwn->bhpn', attn_base, self.mapper)\n",
        "      \n",
        "    def __init__(self, prompts, num_steps: int, cross_replace_steps: float, self_replace_steps: float,\n",
        "                 local_blend: Optional[LocalBlend] = None):\n",
        "        super(AttentionReplace, self).__init__(prompts, num_steps, cross_replace_steps, self_replace_steps, local_blend)\n",
        "        self.mapper = seq_aligner.get_replacement_mapper(prompts, tokenizer).to(device)\n",
        "        \n",
        "\n",
        "class AttentionRefine(AttentionControlEdit):\n",
        "\n",
        "    def replace_cross_attention(self, attn_base, att_replace):\n",
        "        attn_base_replace = attn_base[:, :, self.mapper].permute(2, 0, 1, 3)\n",
        "        attn_replace = attn_base_replace * self.alphas + att_replace * (1 - self.alphas)\n",
        "        return attn_replace\n",
        "\n",
        "    def __init__(self, prompts, num_steps: int, cross_replace_steps: float, self_replace_steps: float,\n",
        "                 local_blend: Optional[LocalBlend] = None):\n",
        "        super(AttentionRefine, self).__init__(prompts, num_steps, cross_replace_steps, self_replace_steps, local_blend)\n",
        "        self.mapper, alphas = seq_aligner.get_refinement_mapper(prompts, tokenizer)\n",
        "        self.mapper, alphas = self.mapper.to(device), alphas.to(device)\n",
        "        self.alphas = alphas.reshape(alphas.shape[0], 1, 1, alphas.shape[1])\n",
        "\n",
        "\n",
        "class AttentionReweight(AttentionControlEdit):\n",
        "\n",
        "    def replace_cross_attention(self, attn_base, att_replace):\n",
        "        if self.prev_controller is not None:\n",
        "            attn_base = self.prev_controller.replace_cross_attention(attn_base, att_replace)\n",
        "        attn_replace = attn_base[None, :, :, :] * self.equalizer[:, None, None, :]\n",
        "        return attn_replace\n",
        "\n",
        "    def __init__(self, prompts, num_steps: int, cross_replace_steps: float, self_replace_steps: float, equalizer,\n",
        "                local_blend: Optional[LocalBlend] = None, controller: Optional[AttentionControlEdit] = None):\n",
        "        super(AttentionReweight, self).__init__(prompts, num_steps, cross_replace_steps, self_replace_steps, local_blend)\n",
        "        self.equalizer = equalizer.to(device)\n",
        "        self.prev_controller = controller\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "def aggregate_attention(attention_store: AttentionStore, res: int, from_where: List[str], is_cross: bool, select: int):\n",
        "    out = []\n",
        "    attention_maps = attention_store.get_average_attention()\n",
        "    num_pixels = res ** 2\n",
        "    for location in from_where:\n",
        "        for item in attention_maps[f\"{location}_{'cross' if is_cross else 'self'}\"]:\n",
        "            if item.shape[1] == num_pixels:\n",
        "                cross_maps = item.reshape(len(prompts), -1, res, res, item.shape[-1])[select]\n",
        "                out.append(cross_maps)\n",
        "    out = torch.cat(out, dim=0)\n",
        "    out = out.sum(0) / out.shape[0]\n",
        "    return out.cpu()\n",
        "\n",
        "\n",
        "def show_cross_attention(attention_store: AttentionStore, res: int, from_where: List[str], select: int = 0):\n",
        "    tokens = tokenizer.encode(prompts[select])\n",
        "    decoder = tokenizer.decode\n",
        "    attention_maps = aggregate_attention(attention_store, res, from_where, True, select)\n",
        "    images = []\n",
        "    for i in range(len(tokens)):\n",
        "        image = attention_maps[:, :, i]\n",
        "        image = 255 * image / image.max()\n",
        "        image = image.unsqueeze(-1).expand(*image.shape, 3)\n",
        "        image = image.numpy().astype(np.uint8)\n",
        "        image = np.array(Image.fromarray(image).resize((256, 256)))\n",
        "        image = ptp_utils.text_under_image(image, decoder(int(tokens[i])))\n",
        "        images.append(image)\n",
        "    ptp_utils.view_images(np.stack(images, axis=0))\n",
        "    \n",
        "\n",
        "def show_self_attention_comp(attention_store: AttentionStore, res: int, from_where: List[str],\n",
        "                        max_com=10, select: int = 0):\n",
        "    attention_maps = aggregate_attention(attention_store, res, from_where, False, select).numpy().reshape((res ** 2, res ** 2))\n",
        "    u, s, vh = np.linalg.svd(attention_maps - np.mean(attention_maps, axis=1, keepdims=True))\n",
        "    images = []\n",
        "    for i in range(max_com):\n",
        "        image = vh[i].reshape(res, res)\n",
        "        image = image - image.min()\n",
        "        image = 255 * image / image.max()\n",
        "        image = np.repeat(np.expand_dims(image, axis=2), 3, axis=2).astype(np.uint8)\n",
        "        image = Image.fromarray(image).resize((256, 256))\n",
        "        image = np.array(image)\n",
        "        images.append(image)\n",
        "    ptp_utils.view_images(np.concatenate(images, axis=1))\n",
        "    \n",
        "def run_and_display(prompts, controller, latent=None, run_baseline=False, generator=None):\n",
        "    if run_baseline:\n",
        "        print(\"w.o. prompt-to-prompt\")\n",
        "        images, latent = run_and_display(prompts, EmptyControl(), latent=latent, run_baseline=False, generator=generator)\n",
        "        print(\"with prompt-to-prompt\")\n",
        "    images, x_t = ptp_utils.text2image_ldm_stable(ldm_stable, prompts, controller, latent=latent, num_inference_steps=NUM_DIFFUSION_STEPS, guidance_scale=GUIDANCE_SCALE, generator=generator, low_resource=LOW_RESOURCE)\n",
        "        \n",
        "    images = ptp_utils.view_images(images)\n",
        "    return images, x_t"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jnOHuWfuBMhY"
      },
      "source": [
        "### Basic generation "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-sXvu9pBMhY"
      },
      "source": [
        "Generate an image and visualze the cross-attention maps for each word in the text prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y7YuE7Sr-bNC"
      },
      "outputs": [],
      "source": [
        "g_cpu = torch.Generator().manual_seed(8888)\n",
        "prompts = [\"a fantasy landscape with a pine forest\"]\n",
        "controller = AttentionStore()\n",
        "image, x_t = run_and_display(prompts, controller, latent=None, run_baseline=False, generator=g_cpu)\n",
        "show_cross_attention(controller, res=16, from_where=(\"up\", \"down\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pzg50CaZ-bND"
      },
      "source": [
        "## Our Code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BAVEMFFLBMhZ"
      },
      "source": [
        "### Attention Remove"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6iCJ8Jur-bNE"
      },
      "outputs": [],
      "source": [
        "def get_equalizer(text: str, word_select: Union[int, Tuple[int, ...]], values: Union[List[float],\n",
        "                  Tuple[float, ...]]):\n",
        "    if type(word_select) is int or type(word_select) is str:\n",
        "        word_select = (word_select,)\n",
        "    equalizer = torch.ones(len(values), 77)\n",
        "    values = torch.tensor(values, dtype=torch.float32)\n",
        "    for word in word_select:\n",
        "        inds = ptp_utils.get_word_inds(text, word, tokenizer)\n",
        "        equalizer[:, inds] = values\n",
        "    return equalizer\n",
        "\n",
        "class AttentionRemove(AttentionControlEdit):\n",
        "    def replace_cross_attention(self, attn_base, att_replace):\n",
        "        self.equalizer = get_equalizer(prompts[1], (self.remove_word,), (-5,))\n",
        "        self.equalizer = self.equalizer.to(device)\n",
        "        if self.prev_controller is not None:\n",
        "            attn_base = self.prev_controller.replace_cross_attention(attn_base, att_replace)\n",
        "        attn_replace = attn_base[None, :, :, :] * self.equalizer[:, None, None, :]\n",
        "        return attn_replace\n",
        "\n",
        "    def __init__(self, prompts, num_steps: int, cross_replace_steps: float, self_replace_steps: float, remove_word,\n",
        "                local_blend: Optional[LocalBlend] = None, controller: Optional[AttentionControlEdit] = None):\n",
        "        super(AttentionRemove, self).__init__(prompts, num_steps, cross_replace_steps, self_replace_steps, local_blend)\n",
        "        self.remove_word = remove_word\n",
        "        self.prev_controller = controller"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wUA3Wqr_BMha"
      },
      "outputs": [],
      "source": [
        "prompts = [\"A kid riding a bicycle with a dog\"] * 2\n",
        "\n",
        "controller = AttentionRemove(prompts, NUM_DIFFUSION_STEPS, cross_replace_steps=.8, remove_word=\"dog\",\n",
        "                               self_replace_steps=.4)\n",
        "_ = run_and_display(prompts, controller, latent=x_t, run_baseline=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YSV8fI6LBMha"
      },
      "outputs": [],
      "source": [
        "prompts = [\"A fantasy landscape with a pine forest\"] * 2\n",
        "\n",
        "controller = AttentionRemove(prompts, NUM_DIFFUSION_STEPS, cross_replace_steps=.8, remove_word=\"fantasy\",\n",
        "                               self_replace_steps=.4)\n",
        "_ = run_and_display(prompts, controller, latent=x_t, run_baseline=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nY7ZI9A1BMhb"
      },
      "outputs": [],
      "source": [
        "prompts = [\"Tofu soup with croutons\"] * 2\n",
        "\n",
        "controller = AttentionRemove(prompts, NUM_DIFFUSION_STEPS, cross_replace_steps=.8, remove_word=\"tofu\",\n",
        "                               self_replace_steps=.4)\n",
        "_ = run_and_display(prompts, controller, latent=x_t, run_baseline=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HrABMPASBMhb"
      },
      "outputs": [],
      "source": [
        "prompts = [\"A photo of a house on a mountain\"] * 2\n",
        "\n",
        "controller = AttentionRemove(prompts, NUM_DIFFUSION_STEPS, cross_replace_steps=.8, remove_word=\"house\",\n",
        "                               self_replace_steps=.4)\n",
        "_ = run_and_display(prompts, controller, latent=x_t, run_baseline=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_v_LQgQBMhc"
      },
      "source": [
        "### Multi-seeds Image Editing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W10GudOx-bNG"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "\n",
        "text to image function, which accepts multiple random seeds instead of just one.\n",
        "\n",
        "\n",
        "modified by Team 8\n",
        "based on the original function text2image_ldm_stable()\n",
        "\n",
        "'''\n",
        "\n",
        "# t8 is for Team 8\n",
        "@torch.no_grad()\n",
        "def text2image_ldm_stable_t8(\n",
        "    model,\n",
        "    prompt_batch: List[str],\n",
        "    seed_batch:List[int],\n",
        "    controller,\n",
        "    num_inference_steps: int = 50,\n",
        "    guidance_scale: float = 7.5,\n",
        "    generator_batch: Optional[torch.Generator] = None,\n",
        "    low_resource: bool = False,\n",
        "):\n",
        "\n",
        "\n",
        "    generator_batch = [torch.Generator().manual_seed(seed) for seed in seed_batch]\n",
        "\n",
        "\n",
        "    ptp_utils.register_attention_control(model, controller)\n",
        "    \n",
        "    height = width = 512\n",
        "    batch_size = len(prompt_batch)\n",
        "\n",
        "    text_input = model.tokenizer(\n",
        "        prompt_batch,\n",
        "        padding=\"max_length\",\n",
        "        max_length=model.tokenizer.model_max_length,\n",
        "        truncation=True,\n",
        "        return_tensors=\"pt\",\n",
        "    )\n",
        "    text_embeddings = model.text_encoder(text_input.input_ids.to(model.device))[0]\n",
        "    max_length = text_input.input_ids.shape[-1]\n",
        "    uncond_input = model.tokenizer(\n",
        "        [\"\"] * batch_size, padding=\"max_length\", max_length=max_length, return_tensors=\"pt\"\n",
        "    )\n",
        "    uncond_embeddings = model.text_encoder(uncond_input.input_ids.to(model.device))[0]\n",
        "    \n",
        "    context = [uncond_embeddings, text_embeddings]\n",
        "    if not low_resource:\n",
        "        context = torch.cat(context)\n",
        "    latent_batch = init_latent_batch_t8(model, height, width, generator_batch)\n",
        "    \n",
        "    # set timesteps\n",
        "    extra_set_kwargs = {\"offset\": 1}\n",
        "    model.scheduler.set_timesteps(num_inference_steps, **extra_set_kwargs)\n",
        "    for t in tqdm(model.scheduler.timesteps):\n",
        "        latent_batch = ptp_utils.diffusion_step(model, controller, latent_batch, context, t, guidance_scale, low_resource)\n",
        "    \n",
        "    image_batch = ptp_utils.latent2image(model.vae, latent_batch)\n",
        "  \n",
        "    return image_batch\n",
        "\n",
        "\n",
        "'''\n",
        "\n",
        "initialize the latent batch based on multiple random seeds\n",
        "\n",
        "\n",
        "modified by Team 8\n",
        "based on the original function init_latent()\n",
        "\n",
        "'''\n",
        "def init_latent_batch_t8( model, height, width, generator_batch):\n",
        "    latent_batch=None\n",
        "\n",
        "    for generator in generator_batch:\n",
        "        latent = torch.randn(\n",
        "            (1, model.unet.in_channels, height // 8, width // 8),\n",
        "            generator=generator,\n",
        "        )\n",
        "\n",
        "        if latent_batch is None:\n",
        "            latent_batch=latent\n",
        "        else:\n",
        "            latent_batch=torch.cat((latent_batch,latent),0)\n",
        "\n",
        "    latent_batch = latent_batch.to(model.device)\n",
        "\n",
        "    return latent_batch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j9DR0eXECcs_"
      },
      "source": [
        "Without local blend"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7HT0_W7VBMhc"
      },
      "outputs": [],
      "source": [
        "prompt_batch = [\"A painting of a squirrel eating a burger\",\n",
        "           \"A painting of a lion eating a burger\",\n",
        "           \"A painting of a lion eating a burger\",\n",
        "           \"A painting of a lion eating a burger\",\n",
        "           ]\n",
        "seed_batch=[8888,9991,232,2344]\n",
        "\n",
        "controller = AttentionReplace(prompt_batch, NUM_DIFFUSION_STEPS, cross_replace_steps=0.8, self_replace_steps=0.4,)\n",
        "\n",
        "\n",
        "image_batch = text2image_ldm_stable_t8(model=ldm_stable, \n",
        "                                       prompt_batch=prompt_batch, \n",
        "                                       controller=controller, \n",
        "                                    num_inference_steps=NUM_DIFFUSION_STEPS, \n",
        "                                    guidance_scale=GUIDANCE_SCALE, \n",
        "                                    low_resource=LOW_RESOURCE,\n",
        "                                    seed_batch=seed_batch\n",
        "                                    )\n",
        "_=ptp_utils.view_images(image_batch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1B3LXYKCgKP"
      },
      "source": [
        "With local blend"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zTMzJzl4jk2a"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "prompt_batch = [\"A painting of a squirrel eating a burger\",\n",
        "           \"A painting of a lion eating a burger\",\n",
        "           \"A painting of a lion eating a burger\",\n",
        "           \"A painting of a lion eating a burger\",\n",
        "           ]\n",
        "\n",
        "seed_batch=[8888,9991,232,2344]\n",
        "\n",
        "lb = LocalBlend(prompt_batch[0:2], (\"squirrel\", \"lion\"))\n",
        "\n",
        "controller = AttentionReplace(prompt_batch, NUM_DIFFUSION_STEPS, cross_replace_steps=0.8, self_replace_steps=0.4, local_blend=lb)\n",
        "\n",
        " \n",
        "image_batch = text2image_ldm_stable_t8(model=ldm_stable, \n",
        "                                       prompt_batch=prompt_batch, \n",
        "                                       controller=controller, \n",
        "                                    num_inference_steps=NUM_DIFFUSION_STEPS, \n",
        "                                    guidance_scale=GUIDANCE_SCALE, \n",
        "                                    low_resource=LOW_RESOURCE,\n",
        "                                    seed_batch=seed_batch\n",
        "                                    )\n",
        "ptp_utils.view_images(image_batch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wyd0lJwA-bNH"
      },
      "source": [
        "### Combine Two Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M3T5nOgg-bNH"
      },
      "outputs": [],
      "source": [
        "def get_combine_mapper_(x: str, y: str, z: str, tokenizer, max_len=77):\n",
        "    words_x = x.split(' ')\n",
        "    words_y = y.split(' ')\n",
        "    words_z = z.split(' ')\n",
        "    inds_replace = [i for i in range(len(words_z)) if words_z[i] != words_x[i]]\n",
        "    inds_x = [seq_aligner.get_word_inds(x, i, tokenizer) for i in inds_replace]\n",
        "    inds_replace_y = [i for i in range(len(words_z)) if words_y[i] != words_x[i]]\n",
        "    inds_y = [seq_aligner.get_word_inds(y, i, tokenizer) for i in inds_replace_y]\n",
        "    inds_z = [seq_aligner.get_word_inds(z, i, tokenizer) for i in inds_replace]\n",
        "    mapper = np.zeros((max_len, max_len))\n",
        "    i = j = k = 0\n",
        "    cur_inds = 0\n",
        "    while i < max_len and j < max_len:\n",
        "        if cur_inds < len(inds_x) and inds_x[cur_inds][0] == i:\n",
        "            inds_x_, inds_y_, inds_z_ = inds_x[cur_inds], inds_y[cur_inds], inds_z[cur_inds]\n",
        "            if len(inds_x_) == len(inds_z_):\n",
        "                mapper[inds_x_, inds_z_] = 1\n",
        "            else:\n",
        "                ratio = 1 / len(inds_y_)\n",
        "                for i_t in inds_z_:\n",
        "                    mapper[inds_x_, i_t] = ratio\n",
        "            cur_inds += 1\n",
        "            i += len(inds_x_)\n",
        "            j += len(inds_z_)\n",
        "            k += len(inds_y_)\n",
        "        elif cur_inds < len(inds_x):\n",
        "            mapper[i, j] = 1\n",
        "            i += 1\n",
        "            j += 1\n",
        "            k += 1\n",
        "        else:\n",
        "            mapper[j, j] = 1\n",
        "            i += 1\n",
        "            j += 1\n",
        "            k += 1\n",
        "    return torch.from_numpy(mapper).float()\n",
        "\n",
        "def combine_mapper(prompts, tokenizer, max_len=77):\n",
        "    x_seq = prompts[0]\n",
        "    y_seq = prompts[1]\n",
        "    z_seq = prompts[2]\n",
        "    mappers = []\n",
        "    mapper_y = seq_aligner.get_replacement_mapper_(x_seq, y_seq, tokenizer, max_len)\n",
        "    mapper_z = get_combine_mapper_(x_seq, y_seq, z_seq, tokenizer, max_len)\n",
        "    # mapper_m = get_combine_mapper_(x_seq, y_seq, tokenizer, max_len)\n",
        "    # mapper_n = get_combine_mapper_(x_seq, z_seq, tokenizer, max_len)\n",
        "    # mapper = seq_aligner.get_replacement_mapper_(mapper_m, mapper_n, tokenizer, max_len)\n",
        "    mappers.append(mapper_y)\n",
        "    mappers.append(mapper_z)\n",
        "    return torch.stack(mappers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CgIFn5db-bNI"
      },
      "outputs": [],
      "source": [
        "class AttentionCombine(AttentionControlEdit):\n",
        "\n",
        "    def replace_cross_attention(self, attn_base, att_replace):\n",
        "        return torch.einsum('hpw,bwn->bhpn', attn_base, self.mapper)\n",
        "      \n",
        "    def __init__(self, prompts, num_steps: int, cross_replace_steps: float, self_replace_steps: float,\n",
        "                 local_blend: Optional[LocalBlend] = None):\n",
        "        super(AttentionCombine, self).__init__(prompts, num_steps, cross_replace_steps, self_replace_steps, local_blend)\n",
        "        self.mapper = combine_mapper(prompts, tokenizer).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UdxbRLb--bNI"
      },
      "outputs": [],
      "source": [
        "prompts = [\"A woman playing soccer\",\n",
        "           \"A man playing basketball\",\n",
        "           \"A woman playing basketball\"]\n",
        "\n",
        "controller = AttentionCombine(prompts, NUM_DIFFUSION_STEPS, cross_replace_steps=.8, self_replace_steps=0.4)\n",
        "_ = run_and_display(prompts, controller, latent=x_t, run_baseline=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZaHXaA-m2zSX"
      },
      "outputs": [],
      "source": [
        "prompts = [\"A red car parking at the Boston street\",\n",
        "           \"A red bike parking at the Seattle street\",\n",
        "           \"A red bike parking at the Boston street\"]\n",
        "\n",
        "controller = AttentionCombine(prompts, NUM_DIFFUSION_STEPS, cross_replace_steps=.8, self_replace_steps=0.4)\n",
        "_ = run_and_display(prompts, controller, latent=x_t, run_baseline=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxUZTWUfDhlr"
      },
      "source": [
        "### Move Object\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gjzOxwOvDkNT"
      },
      "outputs": [],
      "source": [
        "def aggregate_attention_t8(attention_store, res: int, from_where: List[str], is_cross: bool, select: int,prompt_batch):\n",
        "    out = []\n",
        "    attention_maps = attention_store.get_average_attention()\n",
        "    num_pixels = res ** 2\n",
        "    for location in from_where:\n",
        "        for item in attention_maps[f\"{location}_{'cross' if is_cross else 'self'}\"]:\n",
        "            if item.shape[1] == num_pixels:\n",
        "              #print(\"item.shape\",item.shape)# [batch_size*h,res*res,max_num_words]\n",
        "              cross_maps = item.reshape(len(prompt_batch), -1, res, res, item.shape[-1])[select]\n",
        "              #print(\"cross_maps.shape\",cross_maps.shape)# [h,res,res,max_num_words]\n",
        "              \n",
        "              out.append(cross_maps)\n",
        "    out = torch.cat(out, dim=0)\n",
        "\n",
        "\n",
        "    out = out.sum(0) / out.shape[0]\n",
        "\n",
        "    print(\"out.shape\",out.shape)#[res,res,max_num_words]\n",
        "\n",
        "\n",
        "    return out.cpu()\n",
        "\n",
        "\n",
        "def show_cross_attention_t8(controller, resolution: int, from_where: List[str], prompt_index,prompt_batch):\n",
        "\n",
        "\n",
        "  attention_maps = aggregate_attention_t8(controller, resolution, from_where, True, prompt_index,prompt_batch)\n",
        "  \n",
        "  \n",
        "  tokens = tokenizer.encode(prompt_batch[prompt_index])\n",
        "  decoder = tokenizer.decode\n",
        "  images = []\n",
        "  for i in range(len(tokens)):\n",
        "      image = attention_maps[:, :, i]\n",
        "      image = 255 * image / image.max()\n",
        "      image = image.unsqueeze(-1).expand(*image.shape, 3)\n",
        "      image = image.numpy().astype(np.uint8)\n",
        "      image = np.array(Image.fromarray(image).resize((256, 256)))\n",
        "      # image = ptp_utils.text_under_image(image, decoder(int(tokens[i])))\n",
        "      image = text_under_image(image, decoder(int(tokens[i])))\n",
        "      images.append(image)\n",
        "  # ptp_utils.view_images(np.stack(images, axis=0))\n",
        "  view_images(np.stack(images, axis=0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NdvBVIA0D-cE"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "masks_g=None\n",
        "latent_batch_g=None\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class MoveLocalBlendT8:\n",
        "\n",
        "    def __call__(self, x_t, attention_store,remove,move):\n",
        "\n",
        "\n",
        "      \n",
        "\n",
        "\n",
        "      VALID_BATCH_SIZE=3\n",
        "      maps=[]\n",
        "      for attn in attention_store[\"down_cross\"]+ attention_store[\"up_cross\"]:\n",
        "        if attn.shape[1]== 256:\n",
        "          maps.append(attn)\n",
        "    \n",
        "      \n",
        "      # maps = attention_store[\"down_cross\"][2:4] + attention_store[\"up_cross\"][:3];\n",
        "\n",
        "      # print(maps[0].shape)#[24, 256, 77]\n",
        "      # print(len(maps))\n",
        "\n",
        "      h=8\n",
        "\n",
        "      maps = [item[:VALID_BATCH_SIZE*h].reshape(VALID_BATCH_SIZE, -1, 1, 16, 16, MAX_NUM_WORDS) for item in maps];\n",
        "      maps = torch.cat(maps, dim=1);#print(maps.shape) #torch.Size([2, 80, 1, 16, 16, 77])\n",
        "      maps = (maps * self.alpha_layers).sum(-1).mean(1);#print(maps.shape) #torch.Size([2, 1, 16, 16])\n",
        "      \n",
        "      k = 0\n",
        "      masks = nnf.max_pool2d(maps, (k * 2 + 1, k * 2 +1), (1, 1), padding=(k, k));#print(masks.shape)#torch.Size([2, 1, 16, 16])\n",
        "      \n",
        "      \n",
        "      masks = nnf.interpolate(masks, size=(x_t.shape[2:]));#print(masks.shape)#torch.Size([2, 1, 64, 64]) # 把 masks 放大了，和 x_t 相同的尺寸。\n",
        "      masks = masks / masks.max(2, keepdims=True)[0].max(3, keepdims=True)[0];#print(masks.shape)#torch.Size([2, 1, 64, 64]) 把最大值縮放為1。\n",
        "      masks = masks.gt(self.threshold) ;#print(masks.shape)# gt greater 比較閾值, mask 是 true 或者 false#torch.Size([2, 1, 64, 64])\n",
        "      # mask = (mask[:1] + mask[1:]).float();print(mask.shape)#torch.Size([1, 1, 64, 64])\n",
        "\n",
        "      global masks_g, latent_batch_g\n",
        "      masks_g=masks\n",
        "      latent_batch_g=x_t\n",
        "\n",
        "\n",
        "      # x_t shape (batch_size, m=4, res, res)\n",
        "      # x_t = x_t[:1] + mask * (x_t - x_t[:1])\n",
        "\n",
        "      if DEBUG:\n",
        "        plt.imshow(masks_g.cpu().numpy()[0,0])\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "\n",
        "      res=x_t.shape[2]\n",
        "\n",
        "\n",
        "      for i in range(res):\n",
        "        for j in range(res):\n",
        "          if move and masks[0,0,i,j]==True and 0<=self.move_down+i<res and 0<=self.move_right+j<res:\n",
        "\n",
        "            x_t[1,:,i+self.move_down,j+self.move_right]=x_t[0,:,i,j]\n",
        "      \n",
        "      for i in range(res):\n",
        "        for j in range(res):\n",
        "          if remove and masks[0,0,i,j]==True and 0<=self.move_down+i<res and 0<=self.move_right+j<res:\n",
        "\n",
        "            # x_t[1,:,i,j]=x_t[2,:,i,j]\n",
        "            x_t[1,:,i,j]=x_t[0,:,i+self.move_down,j+self.move_right]\n",
        "\n",
        "      return x_t\n",
        "       \n",
        "    def __init__(self, prompt, word, batch_size,move_down,move_right,threshold=0.3):\n",
        "      VALID_BATCH_SIZE=3\n",
        "\n",
        "      alpha_layers = torch.zeros(VALID_BATCH_SIZE,  1, 1, 1, 1, MAX_NUM_WORDS)\n",
        "\n",
        "      ind = get_word_inds(prompt, word, tokenizer)\n",
        "      alpha_layers[:, :, :, :, :, ind] = 1\n",
        "\n",
        "      self.alpha_layers = alpha_layers.to(device)\n",
        "      self.threshold = threshold\n",
        "      self.batch_size=batch_size\n",
        "      self.move_down=move_down\n",
        "      self.move_right=move_right\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oTSCMUoVEKwU"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "def show_self_attention_comp_t8(attention_store, res: int, from_where: List[str],\n",
        "                        max_com=10, select: int = 0,prompt_batch=None):\n",
        "    attention_maps = aggregate_attention_t8(attention_store, res, from_where, False, select,prompt_batch).numpy().reshape((res ** 2, res ** 2))\n",
        "    u, s, vh = np.linalg.svd(attention_maps - np.mean(attention_maps, axis=1, keepdims=True))\n",
        "    images = []\n",
        "    for i in range(max_com):\n",
        "        image = vh[i].reshape(res, res)\n",
        "        image = image - image.min()\n",
        "        image = 255 * image / image.max()\n",
        "        image = np.repeat(np.expand_dims(image, axis=2), 3, axis=2).astype(np.uint8)\n",
        "        image = Image.fromarray(image).resize((256, 256))\n",
        "        image = np.array(image)\n",
        "        images.append(image)\n",
        "    # ptp_utils.view_images(np.concatenate(images, axis=1))\n",
        "    view_images(np.concatenate(images, axis=1))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class MoveControllerT8():\n",
        "\n",
        "    \n",
        "\n",
        "    def __call__(self, attn, is_cross: bool, place_in_unet: str):\n",
        "\n",
        "        if self.control_mode:\n",
        "          attn = self.forward(attn, is_cross, place_in_unet)\n",
        "          self.cur_att_layer += 1\n",
        "          if self.cur_att_layer == self.num_att_layers:\n",
        "            self.cur_att_layer = 0\n",
        "            self.cur_step += 1\n",
        "            self.between_steps()\n",
        "          return attn\n",
        "        else:\n",
        "          return attn\n",
        "      \n",
        "\n",
        "\n",
        "    def forward(self, attn, is_cross: bool, place_in_unet: str):\n",
        "      \n",
        "\n",
        "      # attn [batch_size*h,res*res,max_num_words]\n",
        "\n",
        "\n",
        "      self.store_attention(attn, is_cross, place_in_unet)\n",
        "\n",
        "      # print(\"num_self_replace\",self.num_self_replace)\n",
        "\n",
        "      if is_cross or (self.num_self_replace[0] <= self.cur_step < self.num_self_replace[1]):\n",
        "          h = attn.shape[0] // (self.batch_size)\n",
        "          attn = attn.reshape(self.batch_size, h, *attn.shape[1:])\n",
        "          attn_base, attn_repalce = attn[0], attn[1:]\n",
        "          if is_cross:\n",
        "              alpha_words = self.cross_replace_alpha[self.cur_step]\n",
        "\n",
        "              # print(\"alpha_words\",alpha_words)\n",
        "              attn_repalce_new = self.replace_cross_attention(attn_base, attn_repalce) * alpha_words + (1 - alpha_words) * attn_repalce\n",
        "\n",
        "              attn[1:] = attn_repalce_new\n",
        "          else:\n",
        "              attn[1:] = self.replace_self_attention(attn_base, attn_repalce)\n",
        "          attn = attn.reshape(self.batch_size * h, *attn.shape[2:])\n",
        "      return attn\n",
        "\n",
        "    ######################################################################################\n",
        "\n",
        "    def store_attention(self, attn, is_cross: bool, place_in_unet: str):\n",
        "      key = f\"{place_in_unet}_{'cross' if is_cross else 'self'}\"\n",
        "      if attn.shape[1] <= 32 ** 2:  # avoid memory overhead\n",
        "          self.step_store[key].append(attn)\n",
        "      return attn\n",
        "\n",
        "      \n",
        "    def get_empty_store(self):\n",
        "        return {\"down_cross\": [], \"mid_cross\": [], \"up_cross\": [],\n",
        "                \"down_self\": [],  \"mid_self\": [],  \"up_self\": []}\n",
        "\n",
        "\n",
        "    def between_steps(self): # 把 step_store 的累加到 attention_store 中，然后清空 step_store \n",
        "        if len(self.attention_store) == 0:\n",
        "            self.attention_store = self.step_store\n",
        "        else:\n",
        "            for key in self.attention_store:\n",
        "                for i in range(len(self.attention_store[key])):\n",
        "                    self.attention_store[key][i] += self.step_store[key][i]\n",
        "        self.step_store = self.get_empty_store()\n",
        "\n",
        "    def get_average_attention(self): # 显示 attention map 的时候要调用的\n",
        "        average_attention = {key: [item / self.cur_step for item in self.attention_store[key]] for key in self.attention_store}\n",
        "        return average_attention\n",
        "\n",
        "\n",
        "    #######################################################################################\n",
        "\n",
        "    def step_callback(self, x_t):\n",
        "      remove=False\n",
        "      if self.remove_start_step<=self.cur_step<=self.remove_end_step:\n",
        "        remove=True\n",
        "      move=False\n",
        "      if self.move_start_step<=self.cur_step<=self.move_end_step:\n",
        "        move=True\n",
        "\n",
        "      # remove=False\n",
        "      # if 7<=self.cur_step<=15:\n",
        "      #   remove=True\n",
        "      # move=False\n",
        "      # if 7<=self.cur_step<=15:\n",
        "      #   move=True\n",
        "      if DEBUG:\n",
        "        print(\"current step\",self.cur_step)\n",
        "      x_t = self.local_blend(x_t, self.attention_store,remove=remove,move=move)\n",
        "      return x_t\n",
        "\n",
        "        \n",
        "    def replace_self_attention(self, attn_base, att_replace):\n",
        "        if att_replace.shape[2] <= 16 ** 2:\n",
        "            return attn_base.unsqueeze(0).expand(att_replace.shape[0], *attn_base.shape)\n",
        "        else:\n",
        "            return att_replace\n",
        "\n",
        "\n",
        "    ######################################################################################\n",
        "    def replace_cross_attention(self, attn_base, attn_replace):\n",
        "      attn_replace=attn_replace.clone().detach()\n",
        "\n",
        "      if True:\n",
        "\n",
        "        # if self.cur_step==1: # print to debug\n",
        "        #   print(\"attn_base.shape\",attn_base.shape) #[h,res*res,max_num_words]\n",
        "        # print(\"attn_replace.shape\",attn_replace.shape)#[batch_size-1,h,res*res,max_num_words]\n",
        "\n",
        "        res=int(math.sqrt(attn_replace.shape[2]))\n",
        "        h=attn_replace.shape[1]\n",
        "\n",
        "        map_original=attn_base[:,:,self.word_index].reshape([h,res,res]) \n",
        "        # print(\"image.shape\",image_1.shape)\n",
        "        \n",
        "        map_new=torch.zeros(h, res,res)\n",
        "        map_new2=torch.zeros(h, res,res)\n",
        "\n",
        "\n",
        "\n",
        "        right_move_len=int((self.move_right/64)*res)\n",
        "\n",
        "        map_new[:,:,right_move_len:]=map_original[:,:,0:res-right_move_len]\n",
        "        map_new[:,:,0:right_move_len]=map_original[:,:,res-right_move_len:]\n",
        "\n",
        "        down_move_len=int((self.move_down/64)*res)\n",
        "\n",
        "        map_new2[:,down_move_len:,:]=map_new[:,0:res-down_move_len,:]\n",
        "        map_new2[:,0:down_move_len,:]=map_new[:,res-down_move_len:,:]\n",
        "\n",
        "        \n",
        "        attn_replace[0,:,:,self.word_index]=map_new2.reshape([h,res*res])\n",
        "\n",
        "\n",
        "      self.equalizer = self.equalizer.to(device)\n",
        "\n",
        "      attn_remove = attn_base[None, :, :, :] * self.equalizer[:, None, None, :]\n",
        "      # attn_replace[0]=attn_remove[0]\n",
        "      attn_replace[1]=attn_remove[0]\n",
        "\n",
        "\n",
        "      # print(\"attn_replace.shape\",attn_replace.shape)\n",
        "      return attn_replace\n",
        "\n",
        "      \n",
        "    def __init__(self, \n",
        "                 prompts, \n",
        "                 move_start_step,\n",
        "                 move_end_step,\n",
        "                 remove_start_step,\n",
        "                 remove_end_step,\n",
        "\n",
        "                move_right,\n",
        "                 move_down,\n",
        "                 num_inference_steps: int, \n",
        "                 cross_replace_steps: float, \n",
        "                 self_replace_steps: float,\n",
        "                 local_blend: Optional[LocalBlend] = None,\n",
        "                 word=None,\n",
        "\n",
        "                 \n",
        "                 \n",
        "                 \n",
        "                 ):\n",
        "      \n",
        "        self.cur_step = 0\n",
        "        self.num_att_layers = -1\n",
        "        self.cur_att_layer = 0\n",
        "\n",
        "        #--------------------\n",
        "        self.step_store = self.get_empty_store()\n",
        "        self.attention_store = {}\n",
        "\n",
        "\n",
        "        #--------------------\n",
        "        self.batch_size = len(prompts)\n",
        "        # self.cross_replace_alpha = ptp_utils.get_time_words_attention_alpha(prompts, num_inference_steps, cross_replace_steps, tokenizer).to(device)\n",
        "        self.cross_replace_alpha = get_time_words_attention_alpha(prompts, num_inference_steps, cross_replace_steps, tokenizer).to(device)\n",
        "        \n",
        "        \n",
        "        if type(self_replace_steps) is float:\n",
        "            self_replace_steps = 0, self_replace_steps\n",
        "        self.num_self_replace = int(num_inference_steps * self_replace_steps[0]), int(num_inference_steps * self_replace_steps[1])\n",
        "        self.local_blend = local_blend\n",
        "\n",
        "        self.word_index=get_word_inds(prompts[0],word,tokenizer)[0]\n",
        "        self.equalizer = get_equalizer(prompts[0], (word,), (-5,))\n",
        "        self.move_start_step=int(num_inference_steps*move_start_step)\n",
        "        self.move_end_step=int(num_inference_steps*move_end_step)\n",
        "        self.remove_start_step=int(num_inference_steps*remove_start_step)\n",
        "        self.remove_end_step=int(num_inference_steps*remove_end_step)\n",
        "\n",
        "        # print(self.move_start_step,self.move_end_step,self.remove_start_step,self.remove_end_step)\n",
        "        self.move_right=move_right\n",
        "        self.move_down=move_down\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class MoveEditorT8():\n",
        "\n",
        "\n",
        "\n",
        "  def __init__(self,model):\n",
        "    self.model=model\n",
        "\n",
        "  def move(self, prompt,word,seed,move_down,move_right,local_blend_threshold,num_inference_steps,\n",
        "                 move_start_step=None,\n",
        "                 move_end_step=None,\n",
        "                 remove_start_step=None,\n",
        "                 remove_end_step=None,\n",
        "           cross_replace_steps=None,\n",
        "           \n",
        "           \n",
        "           ):\n",
        "\n",
        "\n",
        "    # Phase One\n",
        "    # num_inference_steps=50\n",
        "    # num_inference_steps=5  # jtcheckpoint\n",
        "    # replace_controller = ReplaceControllerT8([prompt,prompt], num_inference_steps=num_inference_steps, cross_replace_steps=0.8, self_replace_steps=0.4,local_blend=None)\n",
        "\n",
        "    # image_batch = self.prompts_to_images(\n",
        "    #                                    prompt_batch=[prompt,prompt], \n",
        "    #                                    controller=replace_controller, \n",
        "    #                                 num_inference_steps=num_inference_steps, \n",
        "    #                                 guidance_scale=GUIDANCE_SCALE, \n",
        "    #                                 low_resource=LOW_RESOURCE,\n",
        "    #                                 seed_batch=[seed,seed]\n",
        "    #                                 )\n",
        "  \n",
        "    # view_images(image_batch)\n",
        "    # show_cross_attention_t8(replace_controller, resolution=16, from_where=(\"up\", \"down\"),prompt_batch=[prompt,prompt]) # jtcheckpoint\n",
        "    # show_self_attention_comp_t8(replace_controller,16,from_where=(\"up\", \"down\"),select=0,prompt_batch=[prompt,prompt])\n",
        "    \n",
        "    \n",
        "\n",
        "    # Phase Two\n",
        "    num_inference_steps=num_inference_steps  # jtcheckpoint\n",
        "    batch_size=3\n",
        "    prompt_batch=[prompt]*batch_size\n",
        "    seed_batch=[seed]*batch_size\n",
        "\n",
        "    \n",
        "    local_blend = MoveLocalBlendT8(prompt, word,batch_size=batch_size,threshold=local_blend_threshold,move_down=move_down,move_right=move_right)\n",
        "    move_controller = MoveControllerT8(prompt_batch, num_inference_steps=num_inference_steps, cross_replace_steps=cross_replace_steps, self_replace_steps=0.4,local_blend=local_blend,word=word,\n",
        "                 move_start_step=move_start_step,\n",
        "                 move_end_step=move_end_step,\n",
        "                 remove_start_step=remove_start_step,\n",
        "                 remove_end_step=remove_end_step,\n",
        "                 move_right=move_right,\n",
        "                 move_down=move_down,\n",
        "                                       \n",
        "                                       \n",
        "                                       \n",
        "                                       \n",
        "                                       )\n",
        "\n",
        "    image_batch = self.prompts_to_images(\n",
        "                                       prompt_batch=prompt_batch,\n",
        "                                       controller=move_controller, \n",
        "                                    num_inference_steps=num_inference_steps, \n",
        "                                    guidance_scale=GUIDANCE_SCALE, \n",
        "                                    low_resource=LOW_RESOURCE,\n",
        "                                    seed_batch=seed_batch\n",
        "                                    )\n",
        "  \n",
        "    view_images(image_batch)\n",
        "\n",
        "    show_cross_attention_t8(move_controller, resolution=16, from_where=(\"up\", \"down\"),prompt_index=0,prompt_batch=prompt_batch) # jtcheckpoint\n",
        "    show_cross_attention_t8(move_controller, resolution=16, from_where=(\"up\", \"down\"),prompt_index=1,prompt_batch=prompt_batch) # jtcheckpoint\n",
        "    show_cross_attention_t8(move_controller, resolution=16, from_where=(\"up\", \"down\"),prompt_index=2,prompt_batch=prompt_batch) # jtcheckpoint\n",
        "    show_self_attention_comp_t8(move_controller,16,from_where=(\"up\", \"down\"),select=0,prompt_batch=prompt_batch)\n",
        "    show_self_attention_comp_t8(move_controller,16,from_where=(\"up\", \"down\"),select=1,prompt_batch=prompt_batch)\n",
        "\n",
        "\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def prompts_to_images(\n",
        "      self,\n",
        "      prompt_batch: List[str],\n",
        "      seed_batch:List[int],\n",
        "      controller,\n",
        "      num_inference_steps: int = 50,\n",
        "      guidance_scale: float = 7.5,\n",
        "      generator_batch: Optional[torch.Generator] = None,\n",
        "      low_resource: bool = False,\n",
        "  ):\n",
        "\n",
        "\n",
        "      register_attention_control(self.model, controller)\n",
        "      \n",
        "      height = width = 512\n",
        "      batch_size = len(prompt_batch)\n",
        "\n",
        "      text_input = self.model.tokenizer(\n",
        "          prompt_batch,\n",
        "          padding=\"max_length\",\n",
        "          max_length=self.model.tokenizer.model_max_length,\n",
        "          truncation=True,\n",
        "          return_tensors=\"pt\",\n",
        "      )\n",
        "      text_embeddings = self.model.text_encoder(text_input.input_ids.to(self.model.device))[0]\n",
        "      max_length = text_input.input_ids.shape[-1]\n",
        "      uncond_input = self.model.tokenizer(\n",
        "          [\"\"] * batch_size, padding=\"max_length\", max_length=max_length, return_tensors=\"pt\"\n",
        "      )\n",
        "      uncond_embeddings = self.model.text_encoder(uncond_input.input_ids.to(self.model.device))[0] #uncond 是空的\n",
        "      \n",
        "      context = [uncond_embeddings, text_embeddings]\n",
        "\n",
        "      latent_batch = self.init_latent_batch( height, width, seed_batch)\n",
        "      \n",
        "      # set timesteps\n",
        "      extra_set_kwargs = {\"offset\": 1}\n",
        "      self.model.scheduler.set_timesteps(num_inference_steps, **extra_set_kwargs)\n",
        "      for t in tqdm(self.model.scheduler.timesteps):\n",
        "          latent_batch = self.diffusion_step(controller, latent_batch, context, t, guidance_scale, low_resource)\n",
        "      \n",
        "      image_batch = latent2image(self.model.vae, latent_batch)\n",
        "    \n",
        "      return image_batch\n",
        "\n",
        "\n",
        "  def init_latent_batch( self, height, width, seed_batch):\n",
        "      latent_batch=None\n",
        "\n",
        "      generator_batch = [torch.Generator().manual_seed(seed) for seed in seed_batch]\n",
        "\n",
        "      for generator in generator_batch:\n",
        "          latent = torch.randn(\n",
        "              (1, self.model.unet.in_channels, height // 8, width // 8),\n",
        "              generator=generator,\n",
        "          )\n",
        "\n",
        "          if latent_batch is None:\n",
        "              latent_batch=latent\n",
        "          else:\n",
        "              latent_batch=torch.cat((latent_batch,latent),0)\n",
        "\n",
        "      latent_batch = latent_batch.to(self.model.device)\n",
        "\n",
        "      return latent_batch\n",
        "\n",
        "\n",
        "\n",
        "  def diffusion_step(self,controller, latent_batch, context, t, guidance_scale, low_resource=False):\n",
        "\n",
        "    controller.control_mode=False\n",
        "    noise_pred_uncond = self.model.unet(latent_batch, t, encoder_hidden_states=context[0])[\"sample\"] # 空的\n",
        "    \n",
        "    controller.control_mode=True\n",
        "    noise_prediction_text = self.model.unet(latent_batch, t, encoder_hidden_states=context[1])[\"sample\"] \n",
        "\n",
        "    noise_pred = noise_pred_uncond + guidance_scale * (noise_prediction_text - noise_pred_uncond)\n",
        "    latent_batch = self.model.scheduler.step(noise_pred, t, latent_batch)[\"prev_sample\"]\n",
        "    latent_batch = controller.step_callback(latent_batch)\n",
        "    return latent_batch\n",
        "\n",
        "\n",
        "  def get_original_postion(self,word,attention_store):\n",
        "    pass\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bKAlcTUa4KIt"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1WVIKrHAKiRS"
      },
      "outputs": [],
      "source": [
        "DEBUG=False\n",
        "\n",
        "move_editor=MoveEditorT8(model=ldm_stable)\n",
        "\n",
        "# move_editor.move(prompt=\"A photo of a house on a mountain\",\n",
        "#                  word=\"house\",\n",
        "#                  seed=8888,\n",
        "#                  move_down=0,\n",
        "#                  move_right=40,\n",
        "#                  local_blend_threshold=0.65,\n",
        "#                  num_inference_steps=20,\n",
        "#                  move_start_step=0.5,\n",
        "#                  move_end_step=1.0,\n",
        "#                  remove_start_step=0.5,\n",
        "#                  remove_end_step=1.0,\n",
        "                 \n",
        "#                  )\n",
        "\n",
        "\n",
        "move_editor.move(prompt=\"A photo of a house on a mountain\",\n",
        "                 word=\"house\",\n",
        "                 seed=8888,\n",
        "                 move_down=0,\n",
        "                 move_right=40,\n",
        "\n",
        "\n",
        "                 local_blend_threshold=0.7,\n",
        "                 num_inference_steps=15,\n",
        "                 move_start_step=0.5,\n",
        "                 move_end_step=1.0,\n",
        "                 remove_start_step=0.5,\n",
        "                 remove_end_step=1.0,\n",
        "                 cross_replace_steps=1.0\n",
        "                 )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H3cIxYLibW1S"
      },
      "outputs": [],
      "source": [
        "DEBUG=False\n",
        "move_editor=MoveEditorT8(model=ldm_stable)\n",
        "move_editor.move(prompt=\"a strawberry on the ground\",\n",
        "                 word=\"strawberry\",\n",
        "                 seed=888,\n",
        "                 move_down=0,\n",
        "                 move_right=35,\n",
        "\n",
        "                 local_blend_threshold=0.3,\n",
        "                 num_inference_steps=70,\n",
        "                 move_start_step=0.4,\n",
        "                 move_end_step=1.0,\n",
        "                 remove_start_step=0.4,\n",
        "                 remove_end_step=1.0,\n",
        "                 cross_replace_steps=1.0\n",
        "                 )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jCz1tfRs3JZ5"
      },
      "outputs": [],
      "source": [
        "DEBUG=False\n",
        "move_editor=MoveEditorT8(model=ldm_stable)\n",
        "move_editor.move(prompt=\"a dog in the forest\",\n",
        "                 word=\"dog\",\n",
        "                 seed=887,\n",
        "                 move_down=0,\n",
        "                 move_right=20,\n",
        "\n",
        "                 local_blend_threshold=0.4,\n",
        "                 num_inference_steps=40,\n",
        "                 move_start_step=0.4,\n",
        "                 move_end_step=1.0,\n",
        "                 remove_start_step=0.4,\n",
        "                 remove_end_step=1.0,\n",
        "                 cross_replace_steps=1.0\n",
        "                 )"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.8.13 ('640_project')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "4a322bb2e4a8c3481075941bc2c655f0611dba2c5095e6fcb0d29cd853799f64"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
